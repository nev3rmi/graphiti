#!/usr/bin/env python3
"""
Save comprehensive development knowledge to MCP memory - Fixed Version
"""

import subprocess
import time
import json

def run_docker_command(cmd, timeout=30):
    """Helper to run docker commands with timeout"""
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Command timed out"
    except Exception as e:
        return False, "", str(e)

def save_development_knowledge():
    """Save comprehensive development knowledge to MCP memory"""
    print("üß† SAVING DEVELOPMENT KNOWLEDGE TO MCP MEMORY")
    print("=" * 70)
    
    # Development knowledge entries to save
    knowledge_entries = [
        {
            "name": "ollama_integration_best_practices",
            "content": """
OLLAMA INTEGRATION BEST PRACTICES FOR MCP SERVERS

Key Configuration Patterns:
‚Ä¢ Always use OPENAI_BASE_URL=http://[ollama-host]:11434/v1/ for Ollama endpoint
‚Ä¢ Set dummy OPENAI_API_KEY=abc for Ollama (required by OpenAI client but unused)
‚Ä¢ Use MODEL_NAME=deepseek-r1:latest for robust local LLM inference
‚Ä¢ Set EMBEDDER_MODEL_NAME=mxbai-embed-large:latest for quality embeddings
‚Ä¢ Configure EMBEDDING_DIM=1024 to match mxbai-embed-large dimensions

Critical Issues & Solutions:
‚Ä¢ Embedder model mismatch: Default OpenAI configs override Ollama settings in Graphiti
‚Ä¢ Solution: Explicitly configure OpenAIEmbedderConfig with base_url and model params
‚Ä¢ Permission issues in Docker: Use --user root for script execution in containers
‚Ä¢ Memory allocation: Increase semaphore limits for better concurrency with local models

Environment Variable Patterns:
‚Ä¢ NEO4J_URI=neo4j://[host]:7687 for database connection
‚Ä¢ SEMAPHORE_LIMIT=10 for balanced performance vs resource usage
‚Ä¢ GROUP_ID=default for consistent data organization
‚Ä¢ MCP_SERVER_HOST=0.0.0.0 for Docker networking

Performance Optimizations:
‚Ä¢ Use deepseek-r1:latest for fast, capable local inference
‚Ä¢ mxbai-embed-large provides good quality/speed balance for embeddings
‚Ä¢ Configure proper timeouts (30-90 seconds) for LLM operations
‚Ä¢ Monitor Docker resource usage and adjust container limits accordingly
            """,
            "source": "development_knowledge",
            "description": "Best practices for integrating Ollama with MCP servers and Graphiti"
        },
        {
            "name": "graphiti_mcp_development_patterns",
            "content": """
GRAPHITI MCP SERVER DEVELOPMENT PATTERNS & INSIGHTS

Architecture Patterns:
‚Ä¢ FastMCP + Graphiti Core + Neo4j provides robust knowledge graph foundation
‚Ä¢ SSE transport enables real-time AI assistant integration
‚Ä¢ Docker containerization essential for consistent deployment
‚Ä¢ Environment-driven configuration allows flexible model switching

Code Organization Best Practices:
‚Ä¢ Separate tests by category: integration/, unit/, validation/, reports/
‚Ä¢ Use run_tests.py for unified test execution and health monitoring
‚Ä¢ Implement comprehensive health checks for all system components
‚Ä¢ Create status reports for easy system monitoring and debugging

Database Design Insights:
‚Ä¢ Use group_id='default' for consistent data namespacing
‚Ä¢ Episode nodes store conversation content with temporal metadata
‚Ä¢ Entity nodes represent extracted people, organizations, concepts
‚Ä¢ RELATES_TO edges capture relationships between entities
‚Ä¢ Full-text indices enable efficient content search

Error Handling Patterns:
‚Ä¢ Always validate Ollama connectivity before operations
‚Ä¢ Use timeouts for all async operations (30-90 seconds)
‚Ä¢ Implement graceful fallbacks when models are unavailable
‚Ä¢ Log configuration details for easier debugging

Testing Strategies:
‚Ä¢ Direct Neo4j tests avoid complex LLM dependencies
‚Ä¢ Health checks validate all system components independently
‚Ä¢ Memory flow tests verify end-to-end functionality
‚Ä¢ Status reports provide comprehensive system monitoring

Integration Considerations:
‚Ä¢ MCP protocol requires specific tool definitions and schemas
‚Ä¢ SSE transport needs proper CORS and connection handling
‚Ä¢ Neo4j browser provides valuable debugging and data exploration
‚Ä¢ Docker networking requires careful port and host configuration
            """,
            "source": "development_knowledge", 
            "description": "Development patterns and architectural insights for Graphiti MCP servers"
        },
        {
            "name": "troubleshooting_knowledge_base",
            "content": """
GRAPHITI MCP SERVER TROUBLESHOOTING KNOWLEDGE BASE

Common Issues & Solutions:

1. EMBEDDER MODEL MISMATCH
   Problem: "model text-embedding-3-small not found" with Ollama
   Cause: Graphiti defaults to OpenAI models, ignoring custom embedder config
   Solution: Explicitly configure OpenAIEmbedderConfig with base_url and model
   Code Pattern: embedder_config = OpenAIEmbedderConfig(base_url=ollama_url, model="mxbai-embed-large:latest")

2. CONTAINER PERMISSION ISSUES
   Problem: "Permission denied" when running scripts in Docker
   Cause: Non-root user cannot execute copied files
   Solution: Use --user root flag or chmod 755 before execution
   Command: docker exec --user root container_name python3 script.py

3. SERVER INITIALIZATION FAILURES
   Problem: MCP server starts but Graphiti client not initialized
   Cause: Missing environment variables or model unavailability
   Solution: Check logs for specific errors, verify model accessibility
   Debug: docker logs container_name | grep -E "(ERROR|Graphiti|initialized)"

4. NEO4J CONNECTION ISSUES
   Problem: "Connection refused" to Neo4j database
   Cause: Incorrect URI, credentials, or network configuration
   Solution: Test direct connection with curl http://neo4j-host:7474
   Verify: Check NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD environment variables

5. MEMORY OPERATIONS FAILING
   Problem: Episodes not saving or retrieving properly
   Cause: Group ID mismatch, database connectivity, or LLM errors
   Solution: Use group_id='default' consistently, verify database health
   Test: Direct Neo4j queries to verify data storage and retrieval

6. SEARCH FUNCTIONALITY ISSUES
   Problem: Search returns no results despite stored data
   Cause: Embedding model mismatch, index issues, or query problems
   Solution: Verify embedder configuration, rebuild indices if needed
   Fallback: Use keyword-based search with CONTAINS queries

7. DOCKER NETWORKING PROBLEMS
   Problem: Services cannot reach each other
   Cause: Incorrect host names, port mappings, or network configuration
   Solution: Use service names in docker-compose, verify port exposure
   Test: docker exec container_name curl http://service_name:port

8. PERFORMANCE ISSUES
   Problem: Slow response times or timeouts
   Cause: Resource constraints, high concurrency, or model limitations
   Solution: Adjust SEMAPHORE_LIMIT, increase timeouts, monitor resources
   Monitor: docker stats to check CPU/memory usage

Debugging Commands:
‚Ä¢ docker logs --tail 50 mcp_server-graphiti-mcp-1
‚Ä¢ docker exec container_name printenv | grep -E "MODEL|NEO4J|OLLAMA"
‚Ä¢ curl -s http://ollama-host:11434/api/tags
‚Ä¢ python3 tests/reports/system_health_check.py
‚Ä¢ Neo4j browser queries for data verification
            """,
            "source": "development_knowledge",
            "description": "Comprehensive troubleshooting guide for Graphiti MCP server development"
        },
        {
            "name": "deployment_and_scaling_insights",
            "content": """
DEPLOYMENT AND SCALING INSIGHTS FOR GRAPHITI MCP SERVERS

Production Deployment Patterns:

Docker Compose Configuration:
‚Ä¢ Use secrets for sensitive environment variables (API keys, passwords)
‚Ä¢ Configure resource limits: memory, CPU constraints for containers
‚Ä¢ Implement health checks for all services (MCP server, Neo4j, Ollama)
‚Ä¢ Use named volumes for persistent Neo4j data storage
‚Ä¢ Set up log rotation and monitoring for container logs

Environment Management:
‚Ä¢ Create environment-specific .env files (.env.dev, .env.prod, .env.test)
‚Ä¢ Use strong passwords for Neo4j in production environments
‚Ä¢ Implement proper secrets management (Docker secrets, K8s secrets)
‚Ä¢ Configure backup strategies for Neo4j database
‚Ä¢ Monitor disk usage for graph database growth

Performance Optimization:
‚Ä¢ Tune SEMAPHORE_LIMIT based on available resources and model speed
‚Ä¢ Configure Neo4j memory settings: heap size, page cache size
‚Ä¢ Use SSD storage for Neo4j data directory for better I/O performance
‚Ä¢ Monitor Ollama GPU utilization and adjust concurrent requests
‚Ä¢ Implement connection pooling for database connections

Scaling Considerations:
‚Ä¢ Neo4j clustering for high availability and read scaling
‚Ä¢ Multiple Ollama instances behind load balancer for LLM scaling
‚Ä¢ Horizontal scaling of MCP server instances with shared Neo4j backend
‚Ä¢ Content delivery networks for static assets and documentation
‚Ä¢ Message queuing for async processing of large episode batches

Security Best Practices:
‚Ä¢ Network isolation between services using Docker networks
‚Ä¢ Firewall rules restricting access to internal services
‚Ä¢ Regular security updates for base images and dependencies
‚Ä¢ Audit logging for all database and API operations
‚Ä¢ Rate limiting for MCP endpoint access

Monitoring and Observability:
‚Ä¢ Prometheus metrics for system performance monitoring
‚Ä¢ Grafana dashboards for visualizing system health
‚Ä¢ Log aggregation with ELK stack or similar
‚Ä¢ Application performance monitoring (APM) for request tracing
‚Ä¢ Custom health checks for business logic validation

Backup and Recovery:
‚Ä¢ Automated Neo4j database backups with retention policies
‚Ä¢ Configuration backup for environment variables and settings
‚Ä¢ Container image versioning and rollback strategies
‚Ä¢ Data migration scripts for schema updates
‚Ä¢ Disaster recovery procedures and testing

Load Testing Insights:
‚Ä¢ Test concurrent episode ingestion with realistic data volumes
‚Ä¢ Validate search performance with large knowledge graphs
‚Ä¢ Stress test Ollama model serving under high concurrency
‚Ä¢ Monitor memory usage during large batch operations
‚Ä¢ Test recovery behavior after service failures

Cost Optimization:
‚Ä¢ Local processing eliminates external API costs completely
‚Ä¢ Resource right-sizing based on actual usage patterns
‚Ä¢ Scheduled scaling for predictable load patterns
‚Ä¢ Storage optimization with data compression and archival
‚Ä¢ GPU resource sharing for multiple Ollama model serving
            """,
            "source": "development_knowledge",
            "description": "Production deployment and scaling strategies for Graphiti MCP servers"
        },
        {
            "name": "advanced_development_techniques",
            "content": """
ADVANCED DEVELOPMENT TECHNIQUES FOR GRAPHITI MCP SYSTEMS

Custom Entity Type Development:
‚Ä¢ Define domain-specific entity types extending BaseModel with Pydantic
‚Ä¢ Implement custom validation rules for entity extraction
‚Ä¢ Create specialized relationship types for domain knowledge
‚Ä¢ Use entity type filtering for targeted search and retrieval
‚Ä¢ Build entity hierarchies for complex domain modeling

Knowledge Graph Optimization:
‚Ä¢ Design efficient graph schemas for specific use cases
‚Ä¢ Implement graph algorithms for relationship discovery
‚Ä¢ Use graph traversal patterns for contextual search
‚Ä¢ Optimize index strategies for query performance
‚Ä¢ Create materialized views for common query patterns

Advanced Search Implementations:
‚Ä¢ Hybrid search combining semantic similarity and keyword matching
‚Ä¢ Implement relevance scoring with custom algorithms
‚Ä¢ Use graph-based ranking for relationship-aware search
‚Ä¢ Create faceted search with multiple filter dimensions
‚Ä¢ Build auto-complete and suggestion systems

Custom MCP Tool Development:
‚Ä¢ Extend base MCP tools with domain-specific functionality
‚Ä¢ Implement batch operations for bulk data processing
‚Ä¢ Create specialized query tools for complex information retrieval
‚Ä¢ Build analytics tools for knowledge graph insights
‚Ä¢ Design workflow tools for multi-step AI assistant tasks

Integration Architecture Patterns:
‚Ä¢ Multi-tenant systems with group-based data isolation
‚Ä¢ Event-driven architectures with message streaming
‚Ä¢ Microservices patterns for component separation
‚Ä¢ API gateway patterns for unified access control
‚Ä¢ Webhook systems for real-time data synchronization

Data Pipeline Development:
‚Ä¢ ETL pipelines for importing external data sources
‚Ä¢ Data validation and cleaning pipelines
‚Ä¢ Incremental update systems for large datasets
‚Ä¢ Conflict resolution algorithms for concurrent updates
‚Ä¢ Data lineage tracking for audit and debugging

AI Model Integration:
‚Ä¢ Custom embedding models for domain-specific content
‚Ä¢ Fine-tuned LLMs for specialized entity extraction
‚Ä¢ Multi-model systems with model routing and fallbacks
‚Ä¢ Model versioning and A/B testing frameworks
‚Ä¢ Performance monitoring for model drift detection

Testing and Quality Assurance:
‚Ä¢ Property-based testing for graph operations
‚Ä¢ Contract testing for MCP protocol compliance
‚Ä¢ Performance testing with realistic data volumes
‚Ä¢ Chaos engineering for system resilience testing
‚Ä¢ Data quality monitoring and validation

Development Workflow Optimization:
‚Ä¢ Local development environments with Docker Compose
‚Ä¢ CI/CD pipelines with automated testing and deployment
‚Ä¢ Code generation for MCP tool definitions
‚Ä¢ Configuration management with environment templating
‚Ä¢ Documentation generation from code annotations

Debugging and Profiling:
‚Ä¢ Graph visualization tools for knowledge exploration
‚Ä¢ Query profiling and optimization techniques
‚Ä¢ Memory profiling for large graph operations
‚Ä¢ Distributed tracing for multi-service debugging
‚Ä¢ Custom logging strategies for complex workflows
            """,
            "source": "development_knowledge",
            "description": "Advanced development techniques and patterns for sophisticated Graphiti MCP systems"
        }
    ]
    
    # Create script content without template formatting issues
    script_content = '''
import sys
sys.path.insert(0, '/app/.venv/lib/python3.12/site-packages')

try:
    from neo4j import GraphDatabase
    import os
    import time
    import uuid
    import json
    
    uri = os.getenv('NEO4J_URI')
    user = os.getenv('NEO4J_USER')
    password = os.getenv('NEO4J_PASSWORD')
    
    driver = GraphDatabase.driver(uri, auth=(user, password))
    
    # Knowledge entries data
    knowledge_entries = ''' + json.dumps(knowledge_entries, indent=2) + '''
    
    saved_count = 0
    
    with driver.session() as session:
        print("üß† Saving development knowledge entries...")
        
        for entry in knowledge_entries:
            now = int(time.time() * 1000)
            entry_uuid = str(uuid.uuid4())
            
            query = """
            CREATE (e:Episodic {
                uuid: $uuid,
                name: $name,
                content: $content,
                source: $source,
                source_description: $description,
                created_at: $created_at,
                valid_at: $valid_at,
                group_id: 'default'
            })
            RETURN e.uuid as episode_id
            """
            
            result = session.run(query, {
                'uuid': entry_uuid,
                'name': entry['name'],
                'content': entry['content'].strip(),
                'source': entry['source'],
                'description': entry['description'],
                'created_at': now,
                'valid_at': now
            })
            
            record = result.single()
            if record:
                saved_count += 1
                print(f"‚úÖ Saved: {entry['name']}")
                print(f"   UUID: {entry_uuid}")
                print(f"   Content length: {len(entry['content'])} characters")
            else:
                print(f"‚ùå Failed to save: {entry['name']}")
        
        print(f"\\nüìä Knowledge Save Summary:")
        print(f"   Total entries: {len(knowledge_entries)}")
        print(f"   Successfully saved: {saved_count}")
        print(f"   Success rate: {saved_count/len(knowledge_entries)*100:.1f}%")
        
        # Verify searchability
        search_terms = ['Ollama', 'development', 'troubleshooting', 'deployment', 'best practices']
        print(f"\\nüîç Verifying searchability:")
        
        for term in search_terms:
            result = session.run("""
                MATCH (e:Episodic) WHERE e.group_id = 'default'
                AND e.source = 'development_knowledge'
                AND toLower(e.content) CONTAINS toLower($term)
                RETURN count(e) as matches
            """, {'term': term})
            
            matches = result.single()['matches']
            print(f"   {term}: {matches} matches")
        
        # Get total count
        result = session.run("""
            MATCH (e:Episodic) WHERE e.group_id = 'default'
            RETURN count(e) as total
        """)
        total = result.single()['total']
        print(f"\\nüìà Total episodes in database: {total}")
    
    driver.close()
    print(f"\\n‚úÖ Development knowledge successfully saved to MCP memory!")

except Exception as e:
    print(f"‚ùå Failed to save development knowledge: {e}")
    import traceback
    traceback.print_exc()
'''
    
    # Run script in container
    import tempfile
    import os
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
        f.write(script_content)
        temp_file = f.name
    
    try:
        # Copy script to container
        success, stdout, stderr = run_docker_command([
            'docker', 'cp', temp_file, 
            'mcp_server-graphiti-mcp-1:/tmp/save_knowledge_fixed.py'
        ])
        
        if not success:
            print(f"‚ùå Failed to copy script: {stderr}")
            return False
        
        # Run script in container
        success, stdout, stderr = run_docker_command([
            'docker', 'exec', '--user', 'root', 'mcp_server-graphiti-mcp-1',
            'python3', '/tmp/save_knowledge_fixed.py'
        ], timeout=90)
        
        print(stdout)
        if stderr and "warning" not in stderr.lower():
            print(f"Stderr: {stderr}")
        
        return success and "successfully saved" in stdout
        
    finally:
        try:
            os.unlink(temp_file)
        except:
            pass

def main():
    print("üß† SAVING DEVELOPMENT KNOWLEDGE TO MCP MEMORY")
    print("=" * 70)
    print(f"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("Storing comprehensive development insights for future projects")
    
    # Check container
    success, stdout, stderr = run_docker_command([
        'docker', 'ps', '--filter', 'name=mcp_server-graphiti-mcp-1', 
        '--format', '{{.Status}}'
    ])
    
    if not success or "Up" not in stdout:
        print("‚ùå MCP server container is not running")
        return False
    
    print(f"‚úÖ Container status: {stdout.strip()}")
    
    # Save knowledge
    save_success = save_development_knowledge()
    
    print("\\n" + "=" * 70)
    print("üìã DEVELOPMENT KNOWLEDGE STORAGE RESULTS")
    print("=" * 70)
    
    print(f"Save Knowledge:     {'‚úÖ SUCCESS' if save_success else '‚ùå FAILED'}")
    
    if save_success:
        print("\\nüéâ DEVELOPMENT KNOWLEDGE SUCCESSFULLY SAVED!")
        print("‚úÖ Ollama integration best practices stored")
        print("‚úÖ Graphiti MCP development patterns documented")  
        print("‚úÖ Comprehensive troubleshooting guide available")
        print("‚úÖ Deployment and scaling insights preserved")
        print("‚úÖ Advanced development techniques catalogued")
        
        print("\\nüîç Knowledge Access Methods:")
        print("   ‚Ä¢ MCP search: Query for specific topics like 'Ollama troubleshooting'")
        print("   ‚Ä¢ Keyword search: 'best practices', 'deployment', 'performance'")
        print("   ‚Ä¢ Neo4j browser: Filter by source = 'development_knowledge'")
        print("   ‚Ä¢ AI assistant: Ask about specific development challenges")
        
        return True
    else:
        print("\\n‚ùå Failed to save development knowledge")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)